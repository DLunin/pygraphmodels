\documentclass{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[left=75pt]{geometry}
\usepackage{tikz}

\newcommand{\score}[1]{\textrm{score}(#1)}
\newcommand{\bigO}[1]{\mathcal{O}{(#1)}}
\newcommand{\graph}[1]{\mathcal{#1}}
\newcommand{\ops}[1]{\textrm{ops}(#1)}
\newcommand{\expect}[1]{\mathbb{E} #1}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\author{Dmitry Lunin, \\ \small supervised by Senko O.V.}
\title{Improving speed efficiency of score-based Bayesian network structure learning algorithms}
\begin{document}
\maketitle
\tableofcontents
\pagebreak

\abstract{
	The task of Bayesian network structure learning is NP-hard \cite{StructureLearningIsNPComplete}, and as such it is usually solved by heuristic algorithms. That makes speed optimization of the structure learning algorithms especially important, since it allows to traverse more structures in a given time, yielding better quality answers.
	
	The proposed method improves speed efficiency of many structure learning algorithms by optimizing one of the most basic operations in score-based structure learning: the score evaluation and comparison.
}

\section{Structure Learning}
\subsection{Introduction}
\paragraph{The task} The aim of structure learning is to identify the Bayesian network structure (i.e. the graph) using data. 

\paragraph{Applications}

The first application is knowledge discovery. We can use structure learning to find out (in)dependencies between variables in the data, or test our prior assumptions about them, obtaining better understanding of the data and the domain of knowledge. 

The second application is off-the-shelf machine learning. In order to apply Bayesian network methods to a problem, firstly one needs to build a Bayesian network, which may require significant effort and domain expertise. On the other hand, many machine learning methods, for example random forests can be applied immediately, yielding adequate results which can be improved later by fine-tuning parameters. Advanced structure learning methods make it possible to apply such workflow to the Bayesian network methods.

\paragraph{Approaches} There are two main approaches to the structure learning task. The first, constraint-based approach, is based on doing statistical tests to obtain conditional independence statements, and build a Bayesian network that satisfies them. The point of the second approach, the score-based structure learning, is to introduce a metric of compatibility of a Bayesian network with the data, and then optimize it over the space of all Bayesian network structures. In this work, we focus on the score-based approach.

\subsection{Score-based approach}
\paragraph{Description} The idea of score-based approach is to associate a Bayesian network graph with a score, and then maximize it. Since the number of such graphs is very large, greedy optimization methods are often used.
\subsubsection{Decomposable scores}
A score is decomposable if it can be represented as a sum of scores of graph nodes: 
\begin{equation}
\label{eq:decomposable_score}
\score{\graph{G}} = \sum_i{\score{X_i}}
\end{equation} 
A decomposable score allows for fast recalculation of graph score after local operations on graph. 
\subsubsection{Local operations}

\theoremstyle{definition}
\newtheorem*{local.operation}{Def}
\begin{local.operation}
	We call a graph operation \textbf{local} if it affects edges pointing to a constant number of nodes.
\end{local.operation}

\theoremstyle{definition}
\newtheorem*{operation.score}{Def}
\begin{operation.score}
	The \textbf{score} of a graph operation is defined as the difference between the graph scores after and before applying the operation.
	$$ \score{\textrm{op}} = \score{\graph{G}_{\textrm{after}}} - \score{\graph{G}_{\textrm{before}}} $$
\end{operation.score}

Usually the set of operations includes \textbf{edge addition}, \textbf{edge deletion} and \textbf{edge reversal}. 

The advantage of using local operations in the structure search is that their (decomposable) scores can be computed efficiently, since they affect only a constant number of terms in the sum (\ref{eq:decomposable_score}). Edge affection and edge deletion affect 1 term and edge reversal affects 2 terms.

\subsubsection{Speed concerns}
The most expensive operation in the local search is the computation of $\score{X_i}$, especially when the amount of data is large. Hence, we want to minimize the number of such computations.

\subsection{Graph scores and mutual information}
\paragraph{Mutual Information} Mutual information is defined as:
\begin{equation}
I(X; Y) = \sum_{x \in X}{\sum_{y \in Y}{p(x, y)\,\log \frac{p(x,y)}{p(x)\,p(y)}\,dx\,dy}}
\end{equation}
It can also be expressed as $KL$-divergence between distributions $p(x, y)$ and $p(x)p(y)$:
\begin{equation}
I(X; Y) = D_{KL}(p(x, y)\,||\,p(x)p(y))
\end{equation}
Note that $p(x, y) = p(x)p(y)$, and therefore $I(X; Y) = 0$, when $X$ is independent of $Y$. 

\paragraph{Application} Often computing graph scores for structure learning problem requires mutual information estimation. For example, BIC score. The reason behind this is that in discrete case, likelihood of a Bayesian network structure can be computed using mutual information:
\begin{equation}
\log p(\graph{G}|X) = N \sum_{i=1}^M{I(X_i; Pa(X_i))} - N \sum_{i=1}^M{H(X_i)}  
\end{equation}
where $N$ is the number of data points, $M$ is the number of variables.
\paragraph{Entropy} 
Mutual information, on the other hand, can be computed via entropy:
\begin{equation}
\label{eq:mi_via_entropy}
I(X; Y) = H(X) + H(Y) - H(X, Y)
\end{equation}
Where $H(X)$ is Shannon entropy:
\begin{equation}
H(X) = \sum_{x \in X}{p(x)\,\log p(x)\,dx}
\end{equation}
We use Equation \ref{eq:mi_via_entropy} for computing mutual information because it allows us to cache $H(X)$ instead of $I(X; Y)$; since it can be used for computing several $I(X; Y)$, this scheme is more efficient.

\subsection{Greedy Local Search}
\subsubsection{Algorithm description}
At each step of the algorithm, we choose the local operation with maximal score and apply it. The algorithm terminates where there are no operations that increase the score. That means that it has reached a local optimum (relative to the given score and local operations set).

\subsubsection{Improvements}
\paragraph{Storing operation scores in a heap} In order to find an operation with maximal score efficiently, we can use data structures such as a binary heap. That way, we can retrieve an operation with maximum score in $\bigO{1}$ time.

However, when we apply the operation, several problems arise. Firstly, some operations in the heap start violating acyclity constraints. This problem can be solved by checking for acyclity when the operation is retrieved from the heap. 

Secondly, the score of some operations changes. The number of such operations is $\bigO{K}$. Hence removing and re-inserting them to the heap would require $\bigO{K \log{N_{op}}}$ time. 

\paragraph{Tabu search} In order to avoid local maximums and plateaus, a list of recently applied operations can be stored. Operations from that list are not considered during a step of the greedy search.

\paragraph{Dataset perturbation} Another method for avoiding plateaus is to resample the dataset after a certain number of iterations.

\begin{algorithm}[t]
	\caption{Greedy Local Search algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{GreedyLocalSearch}{$\graph{G}_0, \score{\cdot}, \ops{\cdot}$}
		\State $\graph{G} \gets \graph{G}_0$
		\While{$\exists o \in \ops{\graph{G}}: \score{o} > 0$}
		\State $\displaystyle o \gets \argmax_{o \in \ops{\graph{G}}} {\score{o}}$
		\State $\graph{G} \gets o(\graph{G})$
		\EndWhile
		\State \textbf{return} $\graph{G}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\section{Fast score comparison}
\subsection{CPD posterior}
\paragraph{CPD estimation} Suppose we have a discrete random variable which takes on values $v_1, v_2, \ldots, v_m$ with probabilities $p_1, p_2, \ldots, p_m$ (these are the true probabilities). Now we have a dataset $\mathcal{D} = (x_1, x_2, \ldots, x_N)$, and we want to estimate the true probabilities $p_i$.

By the Bayes theorem,
\begin{equation}
p(p_1, \ldots, p_m | \mathcal{D}) = \frac{p(p_1, \ldots, p_m)}{p(\mathcal{D})} p(\mathcal{D} | p_1, \ldots, p_m) 
\end{equation}

$p(\mathcal{D})$ is a constant w.r.t. $p_1, \ldots, p_m$.
\begin{multline}
p(\mathcal{D}|p_1, \ldots, p_m) = \prod_{i=1}^{N}{p(x_i|p_1, \ldots, p_m)} = \prod_{i=1}^{N}{p_{x_i}} = \prod_{i=1}^{N}{p_1^{[x_i = 1]} p_2^{[x_i = 2]} \ldots p_m^{[x_i = m]}} = \\ p_1^{\sum_{i=1}^N{[x_i = 1]}} p_2^{\sum_{i=1}^N{[x_i = 2]}} \ldots p_m^{\sum_{i=1}^N{[x_i = m]}} = p_1^{n_1} p_2^{n_2} \ldots p_m^{n_m} = \prod_{i = 1}^m{p_i^{n_i}}
\end{multline}

We assume that our prior on the probabilities is a Dirichlet distribution, i.e. it has the form 
\begin{equation*}
p(p_1, \ldots, p_m) \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_m)
\end{equation*}

\begin{equation}
p(p_1, \ldots, p_m) = \frac{1}{B(\alpha_1, \alpha_2, \ldots, \alpha_m)} \prod_{i=1}^m{p_i^{\alpha_i - 1}}
\end{equation}

The uniform prior corresponds to $\alpha_i = 1$ assignment.

Combining these equations, we have that the posterior over $p_1, p_2, \ldots, p_m$ is also a Dirichlet distribution
\begin{multline}
p(p_1, p_2, \ldots, p_m|\mathcal{D}) = \frac{1}{Z_1}\,p(p_1, \ldots, p_m)\,p(\mathcal{D}|p_1, \ldots, p_m) = \\ \frac{1}{Z_2} \, \prod_{i=1}^m{p_i^{\alpha_i - 1}} \, \prod_{i=1}^m{p_i^{n_i}} = \frac{1}{Z_2}{ \prod_{i=1}^m{p_i^{n_i + \alpha_i - 1}}}
\end{multline}

\begin{equation*}
p(p_1, p_2, \ldots, p_m|\mathcal{D}) \sim \textrm{Dirichlet}(\alpha_1 + n_1, \alpha_2 + n_2, \ldots, \alpha_m + n_m)
\end{equation*}

\subsection{Dirichlet distribution properties}

\paragraph{Probability density function}
\begin{equation}
p(p_1, \ldots, p_m) = \frac{1}{B(\alpha_1, \alpha_2, \ldots, \alpha_m)} \prod_{i=1}^m{p_i^{\alpha_i - 1}}
\end{equation}

\paragraph{Support}
\begin{equation}
p_1 + p_2 + \ldots + p_m = 1
\end{equation}

\paragraph{Marginals}
\begin{equation*}
\alpha_0 = \sum_{i=1}^m{\alpha_i}
\end{equation*}

\begin{equation*}
p(p_i) \sim \textrm{Beta}(\alpha_i, \alpha_0 - \alpha_i)
\end{equation*}

\begin{equation}
p(p_i) = \frac{1}{B(\alpha_i, \alpha_0 - \alpha_i)} \, p_i^{\alpha_i} (1 - p_i)^{\alpha_0 - \alpha_i}
\end{equation}

\paragraph{Mean}
\begin{equation}
\expect{p_i} = \frac{\alpha_i}{\alpha_0} = \frac{\alpha_i}{\sum\limits_{j=1}^m{\alpha_j}}
\end{equation}

\paragraph{Variance}
\begin{equation}
\mathbb{D}{p_i} = \frac{\alpha_i(\alpha_0 - \alpha_i)}{\alpha_0^2(\alpha_0 + 1)}
\end{equation}

\paragraph{Covariance}
\begin{equation}
\textrm{cov}(p_i, p_j) = \frac{-\alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)}
\end{equation}

\subsection{Theorems}

\newtheorem{theorem}{Theorem}
\begin{theorem}
	If $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_n)$, $x = p_i$, $\alpha_x = \alpha_i$, $y = p_j$, $\alpha_y = \alpha_j$ and $k_x, k_y, m_x, m_y$ are arbitrary positive constants, then 
	\begin{multline}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \,  \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + k_y)}
	\end{multline}
	
	where $\alpha_z = \alpha_0 - \alpha_x - \alpha_y$
	\begin{proof}
		Let $z$ be the combined probability of all values $v_k$ except $v_i$ and $v_j$. Then
		
		\begin{equation*}
		x, y, z \sim \textrm{Dirichlet}(\alpha_x, \alpha_y, \alpha_z)
		\end{equation*}
		
		\begin{equation}
		p(x, y, z) = \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} z^{\alpha_z - 1}
		\end{equation}
		
		
		$x + y + z = 1$, so $z$ is a deterministic function of $x$ and $y$: 
		
		\begin{equation}
		p(x, y) = \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1}
		\end{equation}
		
		\begin{multline*}
		\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \int\limits_{0 < x + y < 1} {p(x, y) \, x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
		\\ \int\limits_{0 < x + y < 1} {\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_{0 < x + y < 1} { x^{\alpha_x + k_x - 1} y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \int\limits_0^{1-x}{ y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log^{m_y} y \, dy}} = 
		\\  \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \int\limits_0^{1-x}{ \frac{\partial^{m_y}}{(\partial \alpha_y)^{m_y}} (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy}} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \frac{\partial^{m_y}}{(\partial \alpha_y)^{m_y}} \int\limits_0^{1-x}{  (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy}} 
		\end{multline*}
		
		Now let's compute $I(x) = \int\limits_0^{1-x}{  (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy} $. Let $y = (1 - x)t$, then
		
		\begin{multline*}
		I(x) = \int\limits_0^1{((1 - x)t)^{\alpha_y + k_y - 1} (1 - x - (1 - x)t)^{\alpha_z - 1} \, d((1 - x)t)} = \\ \int\limits_0^1{(1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, t^{\alpha_y + k_y - 1} (1 - t)^{\alpha_z - 1} \, dt} = \\ (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \int\limits_0^1{t^{\alpha_y + k_y - 1} (1 - t)^{\alpha_z - 1} \, dt} = (1 - x)^{\alpha_y + \alpha_z + k_y - 1} B(\alpha_y + k_y, \alpha_z)
		\end{multline*}
		
		\begin{multline*}
		\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_y}{x} \, \frac{\partial^{m_y} (1 - x)^{\alpha_y + \alpha_z + k_y - 1} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y}} \, dx } = 
		\\ \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} dx }} = 
		\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} dx }} = 
		\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} (x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)}) \, dx }} = 
		\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \, \int\limits_0^1 {x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} \, dx }} = 
		\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \,
			\frac{\partial^{i}}{(\partial \alpha_y)^{i}} \, \int\limits_0^1 {x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, dx }} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \,  \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + k_y)}
		\end{multline*}
	\end{proof}
\end{theorem}

\begin{theorem}
	If $x \sim \textrm{Beta}(\alpha_x, \alpha_y)$, $y = 1 - x$ and $k_x, k_y, m_x, m_y$ are arbitrary positive constants, then 
	\begin{equation}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \frac{1}{B(\alpha_x, \alpha_y)} \, \frac{\partial^{m_x + m_y} \, B(\alpha_x + k_x, \alpha_y + k_y)}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}}
	\end{equation}
	\begin{proof}
		
		\begin{multline*}
		\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = 
		\frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{p(x) x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y dx} =
		\\ \int\limits_0^1{x^{\alpha_x + k_x - 1} y^{\alpha_y + k_y - 1} \log^{m_x} x \log^{m_y} y \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1} \log^{m_x} x \log^{m_y} (1 - x) \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{ \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \, (x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1} \log^{m_y} (1 - x)) \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{ \frac{\partial^{m_x + m_y}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}} \, (x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1}) \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \frac{\partial^{m_x + m_y}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}} \, \int\limits_0^1{  (x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1}) \, dx} = 
		\frac{1}{B(\alpha_x, \alpha_y)} \, \frac{\partial^{m_x + m_y} B(\alpha_x + k_x, \alpha_y + k_y)}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}} 
		\end{multline*}
	\end{proof}
\end{theorem}

\begin{theorem}
	If $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_n)$, $x = p_i$, $\alpha_x = \alpha_i$, $y = p_j$, $\alpha_y = \alpha_j$, then
	
	\begin{multline*}
	\mathbb{E}(x^{k_x}\, y^{k_y} \log^{m_x}{x} \, \log^{m_y}{y} \, \log(x + y)) = 
	\\ -\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z + n)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + n + k_y)}} 
	\end{multline*}	
	
	\begin{proof}
		
		\begin{multline}
		\mathbb{E}(x^{k_x}\, y^{k_y} \log^{m_x}{x} \, \log^{m_y}{y} \, \log(x + y)) = 
		\\ \int\limits_{0 < x + y < 1}{\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(x + y) \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \int\limits_{0 < x + y < 1}{x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(1 - (1 - x - y)) \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \int\limits_{0 < x + y < 1}{x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(-\sum_{n=1}^{\infty}{\frac{(1 - x - y)^n}{n}}) \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy} = 
		\\-\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \int\limits_{0 < x + y < 1}{x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{n + \alpha_z - 1} \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy}}
		\end{multline}
		
		Note that the integral has the same form as in the proof of Theorem 1, so we have
		
		\begin{multline*}
		\mathbb{E}(x^{k_x}\, y^{k_y} \log^{m_x}{x} \, \log^{m_y}{y} \, \log(x + y)) = 
		\\ -\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z + n)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + n + k_y)}} 
		\end{multline*}	
		
	\end{proof}
\end{theorem}

\subsection{Entropy variance}
\paragraph{Entropy} Recall that the entropy has the form
\begin{equation*}
H(p_1, p_2, \ldots, p_m) = \sum_{i = 1}^m{p_i \log p_i}
\end{equation*}

We want to compute the variance of our estimate of the entropy $\mathbb{D}H(p_1, p_2, \ldots, p_m)$, where $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1 + n_1, \ldots, \alpha_m + n_m)$, as discussed above.

\begin{equation}
\mathbb{D}(\sum_{i = 1}^n{X_i}) = \sum_{i=1}^n{\mathbb{D}X_i} + 2 \sum_{i = 1}^n{\sum_{j = 1, \\ j < i}^n{{\textrm{cov}(X_i, X_j)}}}
\end{equation}

Applying this formula to our task, we have that
\begin{equation}
\mathbb{D}(\sum_{i = 1}^n{p_i \log{p_i}}) = \sum_{i=1}^n{\mathbb{D}(p_i \log{p_i})} + 2 \sum_{i = 1}^n{\sum_{j = 1, \\ j < i}^n{{\textrm{cov}(p_i \log{p_i}, p_j \log{p_j})}}}
\end{equation}

A well-known fact from probability theory states that
\begin{equation}
\mathbb{D}X = \mathbb{E}(X - \mathbb{E}X)^2 = \mathbb{E}(X^2 - 2X \mathbb{E}X - (\mathbb{E}X)^2) = \mathbb{E}(X^2) - (\mathbb{E}X)^2
\end{equation}

\begin{multline}
\textrm{cov}(X, Y) = \mathbb{E}(X - \mathbb{E}X)(Y - \mathbb{E}Y) = \mathbb{E}(XY - X \mathbb{E}Y - Y \mathbb{E}X + \mathbb{E}X\mathbb{E}Y) = \mathbb{E}XY - \mathbb{E}X\mathbb{E}Y
\end{multline}

Applying,
\begin{equation}
\mathbb{D}(p_i \log{p_i}) = \mathbb{E}(p_i^2 \log^2{p_i}) - (\mathbb{E} \, p_i \log{p_i})^2 
\end{equation}
\begin{equation}
\textrm{cov}(p_i \log{p_i}, p_j \log{p_j}) = \mathbb{E}(p_i p_j \log{p_i} \log{p_j}) - (\mathbb{E} \, p_i \log{p_i})(\mathbb{E} \, p_j \log{p_j})
\end{equation}

Note that all values on the right side can be computed using Theorem 1 or Theorem 2. Hence, we have an closed-form expression for $\mathbb{D}H(p_1, p_2, \ldots, p_m)$.

\subsection{Mutual information variance}
Mutual information can be expressed as  
\begin{equation}
MI(X, Y) = H(X) + H(Y) - H(X, Y)
\end{equation}
where $X$ and $Y$ are some sets of random variables. 

In local structure search, we need to estimate
\begin{equation}
MI(X, Pa(X)) = H(X) + H(Pa(X)) - H(X, Pa(X))
\end{equation}

While doing structure search, we can easily precompute $H(X)$ on the entire dataset; estimating that term on a subset of the data doesn't make much sense. So we consider $H(X)$ to be a fixed constant as an approximation, which is reasonable because it's variance is very small (it is an entropy over only one variable that is computed using the entire dataset). 

\begin{equation*}
\mathbb{D}(MI(X, Pa(X))) \approx \mathbb{D}(H(Pa(X)) - H(X, Pa(X)))
\end{equation*}

In terms of $p_{11}, p_{12}, \ldots, p_{nm}$ -- probabilities of instantiations of $(X, Pa(X))$:
\begin{equation*}
H(Pa(X)) - H(X, Pa(X)) = -\sum_j{(\sum_i{p_{ij}}) \log(\sum_i{p_{ij}})} + \\ \sum_{i,j}{p_{ij} \log p_{ij}}
\end{equation*}

\begin{equation*}
\mathbb{D}(H(Pa(X)) - H(X, Pa(X))) = \mathbb{D}H(Pa(X)) + \mathbb{D}H(X, Pa(X)) - 2\,\mathrm{cov}(H(Pa(X)), H(X, Pa(X)))
\end{equation*}

Variances $\mathbb{D}H(Pa(X))$ and $\mathbb{D}H(X, Pa(X))$ can be computed as in the previous section. Now consider $\mathrm{cov}(H(Pa(X)), H(X, Pa(X)))$:

\begin{multline}
\mathrm{cov}(H(Pa(X)), H(X, Pa(X))) = \mathrm{cov}(\sum_j{(\sum_i{p_{ij}}) \log(\sum_i{p_{ij}})}, \sum_{i,j}{p_{ij} \log p_{ij}}) = 
\\ \sum_a \sum_{b,c} \mathrm{cov}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}), p_{bc} \log p_{bc})
\end{multline}

\begin{multline}
\mathrm{cov}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}), p_{bc} \log p_{bc}) = \mathbb{E}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}) \, p_{bc} \log p_{bc}) - \mathbb{E}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}})) \mathbb{E}(p_{bc} \log p_{bc})
\end{multline}

The first term can be computed by applying Theorem 3 in case $a = c$:
\begin{multline}
\mathbb{E}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}) \, p_{ba} \log p_{ba}) = \mathbb{E}((p_{ba} + \sum_{i \neq b}{p_{ia}}) \log(p_{ba} + \sum_{i \neq b}{p_{ia}}) \, p_{ba} \log p_{ba}) = \\
\mathbb{E}(p_{ba}^2 \log(p_{ba} + \sum_{i \neq b}{p_{ia}}) \log p_{ba}) + \mathbb{E}((\sum_{i \neq b}{p_{ia}}) \log(p_{ba} + \sum_{i \neq b}{p_{ia}}) \, p_{ba} \log p_{ba}), \,\,\, \mathrm{where} \,\,\, x = p_{ba}, \, y = \sum_{i \neq b}{p_{ia}}
\end{multline}

Or by applying Theorem 1 or Theorem 2 otherwise: $x = p_{bc}, y = \sum_i{p_{ia}}$.

The second term can also be computed using Theorem 1 or Theorem 2.

\subsection{Score difference}
\paragraph{BIC score} A popular score function choice is the Bayesian Information Criterion (BIC) \cite{BICScore}:
\begin{equation}
\textrm{score}_{BIC}(\graph{G}) = \mathit{l}(\graph{G} | \mathcal{D}) - \frac{\log{N}}{2} \textrm{Dim}[\graph{G}] = N \sum_{i=1}^M MI(X_i, \textrm{Pa}(X_i)) + N \sum_{i=1}^M H(X_i) - \frac{\log{N}}{2} \textrm{Dim}[\graph{G}]
\end{equation}

The second term doesn't depend on the network structure, and the last doesn't depend on the data. Hence, we need to consider only the first term (which was done in the previous section).

\paragraph{Edge addition and deletion} Consider addition of an edge from $X_j$ to $X_i$. 
\begin{multline*}
\mathbb{D}(\,\score{X_i; \textrm{Pa}(X_i) \cup \{X_j\}} - \score{X_i; \textrm{Pa}(X_i)}) = 
\\ \mathbb{D}(H(X_i) + H(\textrm{Pa}(X_i), X_j) - H(X_i, \textrm{Pa}(X_i), X_j) - H(X_i) - H(\textrm{Pa}(X_i)) + H(X_i, \textrm{Pa}(X_i))) = 
\\ \mathbb{D}(H(\textrm{Pa}(X_i), X_j) - H(X_i, \textrm{Pa}(X_i), X_j) - H(\textrm{Pa}(X_i)) + H(X_i, \textrm{Pa}(X_i)))
\end{multline*}

In order to this variance, we need to know:
\begin{enumerate}
	\item Variances of individual terms (computed in 2.4)
	\item These covariances can be computed as in 2.5 (one set of variables in the entropy is a subset of another): 
	\\ $cov(H(\textrm{Pa}(X_i), X_j), H(X_i, \textrm{Pa}(X_i), X_j))$, 
	\\ $cov(H(\textrm{Pa}(X_i), X_j),  H(\textrm{Pa}(X_i)))$, 
	\\ $cov(H(X_i, \textrm{Pa}(X_i), X_j), H(\textrm{Pa}(X_i)))$, 
	\\ $cov(H(X_i, \textrm{Pa}(X_i), X_j), H(X_i, \textrm{Pa}(X_i)))$ 
	\\ $cov(H(\textrm{Pa}(X_i)), H(X_i, \textrm{Pa}(X_i)))$
	
	\item $cov(H(\textrm{Pa}(X_i), X_j), H(\textrm{Pa}(X_i), X_i)$, which can't be computed that way.
\end{enumerate}

In order to get an upper bound on $cov(H(\textrm{Pa} (X_i), X_j), H(\textrm{Pa}(X_i), X_i)$, we can use the following statement (via Cauchyâ€“Schwarz inequality):
\begin{equation}
|cov(X, Y)| \le \sqrt{\mathbb{D}X \, \mathbb{D}Y}
\end{equation}

\paragraph{Edge reversal} Edge reversal score variance upper bound can be derived in the same way. Also note that it can be represented as a combination of edge deletion and edge addition.

\section{Improved Greedy Search}
\subsection{Overview}
We focus on the Greedy Local Search algorithm for the following reasons:
\begin{itemize}
	\item It is conceptually simple, but effective. In a large comparative study \cite{MinMaxHillClimbing} its (with certain improvements discussed in Section 1) performance was shown to be on par with other state-of-the-art algorithms.
	\item It is sometimes used as a sub-procedure in more complex structure learning algorithms, such as Min-Max Hill Climbing.
	\item It is very likely that it benefits the most from the proposed improvement. While other algorithms can, for example, employ constraint-based structure learning techniques, Greedy Local Search fully relies on score comparison queries.
\end{itemize}

Estimation of the variance of the operation scores difference allows to estimate \\ $P(\score{\textrm{op}_a} < \score{\textrm{op}_b} | \mathcal{D})$ using Chebyshev inequality.

\subsection{Finding maximum score}
\paragraph{The task} On each step of the Greedy Local Search algorithm, we need to find an operation with the largest score. It can be easily done by linear search in one pass. Now we want to improve the speed of such procedure, while 
\begin{itemize}
	\item getting the correct answer with probability $ \ge 1 - p_0 $.
	\item retaining $\bigO{N_{op}}$ complexity of the algorithm.
\end{itemize}
\paragraph{The algorithm} The algorithm, which we call Stochastic Linear Search, proceeds as follows. As in the ordinary linear search, we consider operations one by one, while maintaining the current maximum. The only difference is that instead of directly comparison of scores computed on the entire dataset, $\textrm{MaxUntilSuccess}$ sub-procedure is used. It either returns the comparison result, or fails if there is not enough data to reliably compare scores.

\begin{algorithm}[t]
	\caption{Stochastic Linear Search}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{MaxUntilSuccess}{$\textrm{op}_1$, $\textrm{op}_2$, $p_0$, $N_{init}$, $N_{batch}$, $\mathcal{D}$}
		\State $N_{cur} \gets N_{init}$
		\While{$N_{cur} < N$}
		\If{$P(\score{\textrm{op}_1} > \score{\textrm{op}_2} \, | \, \mathcal{D}[:N_{cur}]) > p_0$}
			\Return $(\score{\textrm{op}_1} > \score{\textrm{op}_2})$
		\Else \If{$P(\score{\textrm{op}_1} < \score{\textrm{op}_2} \, | \, \mathcal{D}[:N_{cur}]) > p_0$}
		\Return $(\score{\textrm{op}_1} < \score{\textrm{op}_2})$
		\EndIf
		\EndIf
		\State $N_{cur} \gets N_{cur} + N_{batch}$
		\EndWhile 
		
		\Return error
		\EndProcedure
		\\
		
		\Procedure{StochasticLinearSearch}{$\textrm{ops}$, $p_0$, $N_{init}$, $N_{batch}$, $\mathcal{D}$}
		\State $m \gets \textrm{None}$
		\For{$x \in \textrm{ops}$}
		\State $m \gets MaxUntilSuccess(\frac{p_0}{n}, m, x, N_{init}, N_{batch}, \mathcal{D}) $
		\If{$m = \textrm{error}$} 
			\Return error
		\EndIf
		
		\EndFor
		
		\Return $m$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Modified greedy search algorithm}
The only change to the Greedy Local Search algorithm is that instead of simply finding an operation with maximal score the Stochastic Linear Search is used. 

\paragraph{Error probability} As a consequence, on each step of the greedy search, there is (at most) $p_0$ probability to make a suboptimal operation choice. We could try to get an upper bound on the probability of the error of the entire algorithm run, however, it would likely be overestimated. The Chebyshev inequality upper bound of $P(\score{\textrm{op}_a} < \score{\textrm{op}_b} | \mathcal{D})$ is also an overestimation: experiments show that the distribution over operation scores are rather close to normal. On the other hand, we usually don't need precise local greedy search run results per se; rather, we need as high resulting network score as possible, and the fact that it is indeed a local maximum w.r.t. chosen local operations. \textbf{Concluding, it makes sense to leave $p_0$ as an algorithm parameter that controls the accuracy-speed tradeoff}.

\section{Conclusion}
In this work, an optimization of Bayesian network score comparison was proposed, which allows to enhance speed of structure learning algorithms that rely on network score evaluations (i.e. score-based approach methods). It was done by deriving variance of the plug-in entropy and mutual information estimator (which may be a useful result in itself). The necessary modifications to the Greedy Local Search procedure were described. 

\paragraph{Future work}
\begin{itemize}
	\item \textbf{Empirical comparison of the proposed algorithm to the existing ones}.
	\item \textbf{Working with heap} One of the common speed optimizations of the Greedy Local Search is storing operations in a heap. In order to use it alongside the proposed method, a stochastic version of heap is required (like the stochastic version of linear search is used in the current version).
	\item \textbf{Applying the technique to other score-based algorithms} While for some algorithms, like MMHC, it is as simple as plugging in the modified Greedy Search algorithm, others, like GES require further research. 
	\end{itemize}

\begin{thebibliography}{9}
	
	\bibitem{KollerFriedman}
	Daphne Koller, Nir Friedman
	\emph{Probabilistic Graphical Models: Principles and Techniques},
	The MIT Press, Cambridge, Massachusetts,
	2009.
	
	\bibitem{LocalSearch}
	David Heckerman, Dan Geiger, David M. Chickering
	\emph{Learning Bayesian Networks: The Combination of Knowledge and Statistical Data}, 1995
	
	\bibitem{LocalSearchOrigin}
	Edward Herskovits, Gregory Cooper
	\emph{Kutato: An Entropy-Driven System for Construction of Probabilistic Expert Systems from Databases}
	
	\bibitem{MDLScore}
	Lam and F. Bacchus
	\emph{Learning Bayesian belief networks: An approach based on the MDL principle}
	
	\bibitem{BICScore}
	Barron, Rissanen, Yu
	\emph{The minimum description length principle in coding
	and modeling}, 1998
	
	\bibitem{StructureLearningIsNPComplete}
	David M. Chickering. 
	\emph{Learning Bayesian networks is NP-complete}
	
	\bibitem{AsymptoticStructureLearningIsNPHard}
	David M. Chickering, Christopher  Meek, David Heckerman.    \emph{Large-sample   learning   of
	Bayesian networks is NP-hard}
	
	\bibitem{MeekConjecture}
	David M. Chickering
	\emph{Optimal Structure Identification With Greedy Search}, Journal of Machine Learning Research, 2002
	
	\bibitem{MinMaxHillClimbing}
	Ioannis Tsamardinos, Laura E. Brown, Constantin F. Aliferis
	\emph{The max-min hill-climbing Bayesian network
	structure learning algorithm}, 2006
	
	\bibitem{GESLargeSampleOptimality}
	David M. Chickering, Cristopher Meek
	\emph{Finding Optimal Bayesian Networks}, UAI, 2002
	
	\bibitem{Review}
	Timo J. T. Koski, John M. Noble
	\emph{A Review of Bayesian Networks and Structure Learning}
	
\end{thebibliography}

\end{document}

