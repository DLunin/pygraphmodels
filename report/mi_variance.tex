\documentclass{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[left=50pt]{geometry}
\usepackage{tikz}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother
%

\providecommand\given{} % so it exists
\newcommand\SetSymbol[1][]{
	\nonscript\,#1\vert \allowbreak \nonscript\,\mathopen{}}

\DeclarePairedDelimiterX\Set[1]{\lbrace}{\rbrace}%
{ \renewcommand\given{\SetSymbol[\delimsize]} #1 }

\newcommand{\pd}[2]{
	\frac{\partial{#1}}{\partial{#2}}
}

\newcommand{\ppd}[1]{
	\frac{\partial}{\partial{#1}}
}

\newcommand{\mean}[3]{
	\frac{1}{#2} \sum_{#1 = 1}^{#2}{#3}
}

\newcommand{\score}[1]{
	\textrm{score}(#1)
}

\newcommand{\pa}[1]{Pa(#1)}

\newcommand{\affnodes}[1]{
	\textrm{Aff}(#1)	
}

\newcommand{\addedge}[1]{
	\textrm{Add}(#1)	
}

\newcommand{\rmedge}[1]{
	\textrm{Del}(#1)
}

\newcommand{\revedge}[1]{
	\textrm{Rev}(#1)
}

\newcommand{\bigO}[1]{
	\mathcal{O}{(#1)}
}

\newcommand{\graph}[1]{
	\mathcal{#1}
}

\newcommand{\var}[1]{{\ttfamily#1}}% variable

\newcommand{\ops}[1]{
	\textrm{ops}(#1)
}

\newcommand{\expect}[1]{
	\mathbb{E} #1
}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\author{Dmitry Lunin}
\title{Estimating variance of the mutual information}
\begin{document}
	\maketitle

\section{CPD posterior}
\paragraph{CPD estimation} Suppose we have a discrete random variable which takes on values $v_1, v_2, \ldots, v_m$ with probabilities $p_1, p_2, \ldots, p_m$ (these are the true probabilities). Now we have a dataset $\mathcal{D} = (x_1, x_2, \ldots, x_N)$, and we want to estimate the true probabilities $p_i$.

By the Bayes theorem,
\begin{equation}
p(p_1, \ldots, p_m | \mathcal{D}) = \frac{p(p_1, \ldots, p_m)}{p(\mathcal{D})} p(\mathcal{D} | p_1, \ldots, p_m) 
\end{equation}

$p(\mathcal{D})$ is a constant w.r.t. $p_1, \ldots, p_m$.
\begin{multline}
p(\mathcal{D}|p_1, \ldots, p_m) = \prod_{i=1}^{N}{p(x_i|p_1, \ldots, p_m)} = \prod_{i=1}^{N}{p_{x_i}} = \prod_{i=1}^{N}{p_1^{[x_i = 1]} p_2^{[x_i = 2]} \ldots p_m^{[x_i = m]}} = \\ p_1^{\sum_{i=1}^N{[x_i = 1]}} p_2^{\sum_{i=1}^N{[x_i = 2]}} \ldots p_m^{\sum_{i=1}^N{[x_i = m]}} = p_1^{n_1} p_2^{n_2} \ldots p_m^{n_m} = \prod_{i = 1}^m{p_i^{n_i}}
\end{multline}

We assume that our prior on the probabilities is a Dirichlet distribution, i.e. it has the form 
\begin{equation*}
p(p_1, \ldots, p_m) \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_m)
\end{equation*}

\begin{equation}
p(p_1, \ldots, p_m) = \frac{1}{B(\alpha_1, \alpha_2, \ldots, \alpha_m)} \prod_{i=1}^m{p_i^{\alpha_i - 1}}
\end{equation}

The uniform prior corresponds to $\alpha_i = 1$ assignment.

Combining these equations, we have that the posterior over $p_1, p_2, \ldots, p_m$ is also a Dirichlet distribution
\begin{multline}
p(p_1, p_2, \ldots, p_m|\mathcal{D}) = \frac{1}{Z_1}\,p(p_1, \ldots, p_m)\,p(\mathcal{D}|p_1, \ldots, p_m) = \\ \frac{1}{Z_2} \, \prod_{i=1}^m{p_i^{\alpha_i - 1}} \, \prod_{i=1}^m{p_i^{n_i}} = \frac{1}{Z_2}{ \prod_{i=1}^m{p_i^{n_i + \alpha_i - 1}}}
\end{multline}

\begin{equation*}
p(p_1, p_2, \ldots, p_m|\mathcal{D}) \sim \textrm{Dirichlet}(\alpha_1 + n_1, \alpha_2 + n_2, \ldots, \alpha_m + n_m)
\end{equation*}

\section{Dirichlet distribution}

\paragraph{Probability density function}
\begin{equation}
p(p_1, \ldots, p_m) = \frac{1}{B(\alpha_1, \alpha_2, \ldots, \alpha_m)} \prod_{i=1}^m{p_i^{\alpha_i - 1}}
\end{equation}

\paragraph{Support}
\begin{equation}
p_1 + p_2 + \ldots + p_m = 1
\end{equation}

\paragraph{Marginals}
\begin{equation*}
\alpha_0 = \sum_{i=1}^m{\alpha_i}
\end{equation*}

\begin{equation*}
p(p_i) \sim \textrm{Beta}(\alpha_i, \alpha_0 - \alpha_i)
\end{equation*}

\begin{equation}
p(p_i) = \frac{1}{B(\alpha_i, \alpha_0 - \alpha_i)} \, p_i^{\alpha_i} (1 - p_i)^{\alpha_0 - \alpha_i}
\end{equation}

\paragraph{Mean}
\begin{equation}
\expect{p_i} = \frac{\alpha_i}{\alpha_0} = \frac{\alpha_i}{\sum\limits_{j=1}^m{\alpha_j}}
\end{equation}

\paragraph{Variance}
\begin{equation}
\mathbb{D}{p_i} = \frac{\alpha_i(\alpha_0 - \alpha_i)}{\alpha_0^2(\alpha_0 + 1)}
\end{equation}

\paragraph{Covariance}
\begin{equation}
\textrm{cov}(p_i, p_j) = \frac{-\alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)}
\end{equation}

\section{Theorems}

\newtheorem{theorem}{Theorem}
\begin{theorem}
If $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_n)$, $x = p_i$, $\alpha_x = \alpha_i$, $y = p_j$, $\alpha_y = \alpha_j$ and $k_x, k_y, m_x, m_y$ are arbitrary nonnegative constants, then 
\begin{multline}
\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \,  \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + k_y)}
\end{multline}

where $\alpha_z = \alpha_0 - \alpha_x - \alpha_y$
\begin{proof}
	Let $z$ be the combined probability of all values $v_k$ except $v_i$ and $v_j$. Then
	
	\begin{equation*}
	x, y, z \sim \textrm{Dirichlet}(\alpha_x, \alpha_y, \alpha_z)
	\end{equation*}
	
	\begin{equation}
	p(x, y, z) = \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} z^{\alpha_z - 1}
	\end{equation}
	
	
	$x + y + z = 1$, so $z$ is a deterministic function of $x$ and $y$: 
	
	\begin{equation}
	p(x, y) = \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1}
	\end{equation}
	
	\begin{multline*}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \int\limits_{0 < x + y < 1} {p(x, y) \, x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
	\\ \int\limits_{0 < x + y < 1} {\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_{0 < x + y < 1} { x^{\alpha_x + k_x - 1} y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \int\limits_0^{1-x}{ y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log^{m_y} y \, dy}} = 
	\\  \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \int\limits_0^{1-x}{ \frac{\partial^{m_y}}{(\partial \alpha_y)^{m_y}} (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy}} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \frac{\partial^{m_y}}{(\partial \alpha_y)^{m_y}} \int\limits_0^{1-x}{  (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy}} 
	\end{multline*}
	
	Now let's compute $I(x) = \int\limits_0^{1-x}{  (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy} $. Let $y = (1 - x)t$, then
	
	\begin{multline*}
	I(x) = \int\limits_0^1{((1 - x)t)^{\alpha_y + k_y - 1} (1 - x - (1 - x)t)^{\alpha_z - 1} \, d((1 - x)t)} = \\ \int\limits_0^1{(1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, t^{\alpha_y + k_y - 1} (1 - t)^{\alpha_z - 1} \, dt} = \\ (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \int\limits_0^1{t^{\alpha_y + k_y - 1} (1 - t)^{\alpha_z - 1} \, dt} = (1 - x)^{\alpha_y + \alpha_z + k_y - 1} B(\alpha_y + k_y, \alpha_z)
	\end{multline*}
	
	\begin{multline*}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_y}{x} \, \frac{\partial^{m_y} (1 - x)^{\alpha_y + \alpha_z + k_y - 1} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y}} \, dx } = 
	\\ \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} (x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)}) \, dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \, \int\limits_0^1 {x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} \, dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \,
		\frac{\partial^{i}}{(\partial \alpha_y)^{i}} \, \int\limits_0^1 {x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, dx }} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \,  \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + k_y)}
	\end{multline*}
\end{proof}
\end{theorem}

\begin{theorem}
	If $x \sim \textrm{Beta}(\alpha_x, \alpha_y)$, $y = 1 - x$ and $k_x, k_y, m_x, m_y$ are arbitrary nonnegative constants, then 
	\begin{equation}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \frac{1}{B(\alpha_x, \alpha_y)} \, \frac{\partial^{m_x + m_y} \, B(\alpha_x + k_x, \alpha_y + k_y)}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}}
	\end{equation}
	\begin{proof}
		
		\begin{multline*}
		\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = 
		\frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{p(x) x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y dx} = \int\limits_0^1{x^{\alpha_x + k_x - 1} y^{\alpha_y + k_y - 1} \log^{m_x} x \log^{m_y} y \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1} \log^{m_x} x \log^{m_y} (1 - x) \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{ \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \, (x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1} \log^{m_y} (1 - x)) \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \int\limits_0^1{ \frac{\partial^{m_x + m_y}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}} \, (x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1}) \, dx} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y)} \, \frac{\partial^{m_x + m_y}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}} \, \int\limits_0^1{  (x^{\alpha_x + k_x - 1} (1 - x)^{\alpha_y + k_y - 1}) \, dx} = 
		\frac{1}{B(\alpha_x, \alpha_y)} \, \frac{\partial^{m_x + m_y} B(\alpha_x + k_x, \alpha_y + k_y)}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{m_y}} 
		\end{multline*}
	\end{proof}
\end{theorem}

\begin{theorem}
	If $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_n)$, $x = p_i$, $\alpha_x = \alpha_i$, $y = p_j$, $\alpha_y = \alpha_j$, then
	
	\begin{multline*}
	\mathbb{E}(x^{k_x}\, y^{k_y} \log^{m_x}{x} \, \log^{m_y}{y} \, \log(x + y)) = 
	\\ -\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z + n)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + n + k_y)}} 
	\end{multline*}	
	
	\begin{proof}
		
		\begin{multline}
		\mathbb{E}(x^{k_x}\, y^{k_y} \log^{m_x}{x} \, \log^{m_y}{y} \, \log(x + y)) = 
		\\ \int\limits_{0 < x + y < 1}{\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(x + y) \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \int\limits_{0 < x + y < 1}{x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(1 - (1 - x - y)) \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy} = 
		\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \int\limits_{0 < x + y < 1}{x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(-\sum_{n=1}^{\infty}{\frac{(1 - x - y)^n}{n}}) \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy} = 
		\\-\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \int\limits_{0 < x + y < 1}{x^{k_x + \alpha_x - 1} y^{k_y + \alpha_y - 1} (1 - x - y)^{n + \alpha_z - 1} \, \log^{m_x}{x} \, \log^{m_y}{y} \, dx \, dy}}
		\end{multline}
		
		Note that the integral has the same form as in the proof of Theorem 1, so we have
		
		\begin{multline*}
		\mathbb{E}(x^{k_x}\, y^{k_y} \log^{m_x}{x} \, \log^{m_y}{y} \, \log(x + y)) = 
		\\ -\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z + n)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + n + k_y)}} 
		\end{multline*}	
		
	\end{proof}
\end{theorem}

\section{Entropy variance}
\paragraph{Entropy} Recall that the entropy has the form
\begin{equation*}
H(p_1, p_2, \ldots, p_m) = \sum_{i = 1}^m{p_i \log p_i}
\end{equation*}

We want to compute the variance of our estimate of the entropy $\mathbb{D}H(p_1, p_2, \ldots, p_m)$, where $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1 + n_1, \ldots, \alpha_m + n_m)$, as discussed above.

\begin{equation}
\mathbb{D}(\sum_{i = 1}^n{X_i}) = \sum_{i=1}^n{\mathbb{D}X_i} + 2 \sum_{i = 1}^n{\sum_{j = 1, \\ j < i}^n{{\textrm{cov}(X_i, X_j)}}}
\end{equation}

Applying this formula to our task, we have that
\begin{equation}
\mathbb{D}(\sum_{i = 1}^n{p_i \log{p_i}}) = \sum_{i=1}^n{\mathbb{D}(p_i \log{p_i})} + 2 \sum_{i = 1}^n{\sum_{j = 1, \\ j < i}^n{{\textrm{cov}(p_i \log{p_i}, p_j \log{p_j})}}}
\end{equation}

A well-known fact from probability theory states that
\begin{equation}
\mathbb{D}X = \mathbb{E}(X - \mathbb{E}X)^2 = \mathbb{E}(X^2 - 2X \mathbb{E}X - (\mathbb{E}X)^2) = \mathbb{E}(X^2) - (\mathbb{E}X)^2
\end{equation}

\begin{multline}
\textrm{cov}(X, Y) = \mathbb{E}(X - \mathbb{E}X)(Y - \mathbb{E}Y) = \mathbb{E}(XY - X \mathbb{E}Y - Y \mathbb{E}X + \mathbb{E}X\mathbb{E}Y) = \mathbb{E}XY - \mathbb{E}X\mathbb{E}Y
\end{multline}

Applying,
\begin{equation}
\mathbb{D}(p_i \log{p_i}) = \mathbb{E}(p_i^2 \log^2{p_i}) - (\mathbb{E} \, p_i \log{p_i})^2 
\end{equation}
\begin{equation}
\textrm{cov}(p_i \log{p_i}, p_j \log{p_j}) = \mathbb{E}(p_i p_j \log{p_i} \log{p_j}) - (\mathbb{E} \, p_i \log{p_i})(\mathbb{E} \, p_j \log{p_j})
\end{equation}

Note that all values on the right side can be computed using Theorem 1 or Theorem 2. Hence, we have an closed-form expression for $\mathbb{D}H(p_1, p_2, \ldots, p_m)$.

\section{Mutual information variance}
Mutual information can be expressed as  
\begin{equation}
MI(X, Y) = H(X) + H(Y) - H(X, Y)
\end{equation}
where $X$ and $Y$ are some sets of random variables. 

In local structure search, we need to estimate
\begin{equation}
MI(X, Pa(X)) = H(X) + H(Pa(X)) - H(X, Pa(X))
\end{equation}

While doing structure search, we can easily precompute $H(X)$ on the entire dataset; estimating that term on a subset of the data doesn't make much sense. So we consider $H(X)$ to be a fixed constant as an approximation, which is reasonable because it's variance is very small (it is an entropy over only one variable that is computed using the entire dataset). 

\begin{equation*}
\mathbb{D}(MI(X, Pa(X))) \approx \mathbb{D}(H(Pa(X)) - H(X, Pa(X)))
\end{equation*}

In terms of $p_{11}, p_{12}, \ldots, p_{nm}$ -- probabilities of instantiations of $(X, Pa(X))$:
\begin{equation*}
H(Pa(X)) - H(X, Pa(X)) = -\sum_j{(\sum_i{p_{ij}}) \log(\sum_i{p_{ij}})} + \\ \sum_{i,j}{p_{ij} \log p_{ij}}
\end{equation*}

\begin{equation*}
\mathbb{D}(H(Pa(X)) - H(X, Pa(X))) = \mathbb{D}H(Pa(X)) + \mathbb{D}H(X, Pa(X)) - 2\,\mathrm{cov}(H(Pa(X)), H(X, Pa(X)))
\end{equation*}

Variances $\mathbb{D}H(Pa(X))$ and $\mathbb{D}H(X, Pa(X))$ can be computed as in the previous section. Now consider $\mathrm{cov}(H(Pa(X)), H(X, Pa(X)))$:

\begin{multline}
\mathrm{cov}(H(Pa(X)), H(X, Pa(X))) = \mathrm{cov}(\sum_j{(\sum_i{p_{ij}}) \log(\sum_i{p_{ij}})}, \sum_{i,j}{p_{ij} \log p_{ij}}) = 
\\ \sum_a \sum_{b,c} \mathrm{cov}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}), p_{bc} \log p_{bc})
\end{multline}

\begin{multline}
\mathrm{cov}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}), p_{bc} \log p_{bc}) = \mathbb{E}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}) \, p_{bc} \log p_{bc}) - \mathbb{E}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}})) \mathbb{E}(p_{bc} \log p_{bc})
\end{multline}

The first term can be computed by applying Theorem 3 in case $a = c$:
\begin{multline}
\mathbb{E}((\sum_i{p_{ia}}) \log(\sum_i{p_{ia}}) \, p_{ba} \log p_{ba}) = \mathbb{E}((p_{ba} + \sum_{i \neq b}{p_{ia}}) \log(p_{ba} + \sum_{i \neq b}{p_{ia}}) \, p_{ba} \log p_{ba}) = \\
\mathbb{E}(p_{ba}^2 \log(p_{ba} + \sum_{i \neq b}{p_{ia}}) \log p_{ba}) + \mathbb{E}((\sum_{i \neq b}{p_{ia}}) \log(p_{ba} + \sum_{i \neq b}{p_{ia}}) \, p_{ba} \log p_{ba}), \,\,\, \mathrm{where} \,\,\, x = p_{ba}, \, y = \sum_{i \neq b}{p_{ia}}
\end{multline}

Or by applying Theorem 1 or Theorem 2 otherwise: $x = p_{bc}, y = \sum_i{p_{ia}}$.

The second term can also be computed using Theorem 1 or Theorem 2.

\end{document}