\documentclass{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[left=50pt]{geometry}
\usepackage{tikz}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother
%

\providecommand\given{} % so it exists
\newcommand\SetSymbol[1][]{
	\nonscript\,#1\vert \allowbreak \nonscript\,\mathopen{}}

\DeclarePairedDelimiterX\Set[1]{\lbrace}{\rbrace}%
{ \renewcommand\given{\SetSymbol[\delimsize]} #1 }

\newcommand{\pd}[2]{
	\frac{\partial{#1}}{\partial{#2}}
}

\newcommand{\ppd}[1]{
	\frac{\partial}{\partial{#1}}
}

\newcommand{\mean}[3]{
	\frac{1}{#2} \sum_{#1 = 1}^{#2}{#3}
}

\newcommand{\score}[1]{
	\textrm{score}(#1)
}

\newcommand{\pa}[1]{Pa(#1)}

\newcommand{\affnodes}[1]{
	\textrm{Aff}(#1)	
}

\newcommand{\addedge}[1]{
	\textrm{Add}(#1)	
}

\newcommand{\rmedge}[1]{
	\textrm{Del}(#1)
}

\newcommand{\revedge}[1]{
	\textrm{Rev}(#1)
}

\newcommand{\bigO}[1]{
	\mathcal{O}{(#1)}
}

\newcommand{\graph}[1]{
	\mathcal{#1}
}

\newcommand{\var}[1]{{\ttfamily#1}}% variable

\newcommand{\ops}[1]{
	\textrm{ops}(#1)
}

\newcommand{\expect}[1]{
	\mathbb{E} #1
}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\author{Dmitry Lunin}
\title{Bayesian Networks}
\begin{document}
\maketitle

\section{Overview}
\subsection{Definition}

\subsection{Workflow}
\begin{tikzpicture}[line width=2pt]
\draw (0,0) node [above right]  {$A$} -- (1,0);

\end{tikzpicture}

\section{Structure Learning}
\subsection{Introduction}
\paragraph{The task} The aim of structure learning is to identify the Bayesian network structure (i.e. the graph) using data. 

\paragraph{Applications}

The first application is knowledge discovery. We can use structure learning to find out (in)dependencies between variables in the data, or test our prior assumptions about them, obtaining better understanding of the data and the domain of knowledge. 

The second application is off-the-shelf machine learning. In order to apply Bayesian network methods to a problem, firstly one needs to build a Bayesian network, which may require significant effort and domain expertise. On the other hand, many machine learning methods, for example random forests can be applied immediately, yielding adequate results which can be improved later by fine-tuning parameters. Advanced structure learning methods make it possible to apply such workflow to the Bayesian network methods.

\paragraph{Approaches} There are two main approaches to the structure learning task. The first, constraint-based approach, is based on doing statistical tests to obtain conditional independence statements, and build a Bayesian network that satisfies them. The point of the second approach, the score-based structure learning, is to introduce a metric of compatibility of a Bayesian network with the data, and then optimize it over the space of all Bayesian network structures. In this work, we focus on the score-based approach.

\subsection{Score-based approach}
\paragraph{Description} The idea of score-based approach is to associate a Bayesian network graph with a score, and then maximize it. Since the number of such graphs is very large, greedy optimization methods are often used.
\subsubsection{Decomposable scores}
A score is decomposable if it can be represented as a sum of scores of graph nodes: $$ \score{\graph{G}} = \sum_i{\score{X_i}} $$
A decomposable score allows for fast recalculation of graph score after local operations on graph. 
\subsubsection{Local operations}

\theoremstyle{definition}
\newtheorem*{local.operation}{Def}
\begin{local.operation}
	We call a graph operation \textbf{local} if it affects edges pointing to a constant number of nodes. We will denote the set of affected nodes as $\affnodes{\textrm{op}}$.
\end{local.operation}

\theoremstyle{definition}
\newtheorem*{operation.score}{Def}
\begin{operation.score}
	The \textbf{score} of a graph operation is defined as the difference between the graph scores after and before applying the operation.
	$$ \score{\textrm{op}} = \score{\graph{G}_{\textrm{after}}} - \score{\graph{G}_{\textrm{before}}} $$
\end{operation.score}

\paragraph{Edge Addition}
Add edge from node $u$ to node $v$. Denoted as $\addedge{u, v}$. $$ \affnodes{\addedge{u, v}} = \Set{v} $$
\paragraph{Edge Deletion}
Delete edge from node $u$ to node $v$. Denoted as $\rmedge{u, v}$. $$ \affnodes{\rmedge{u, v}} = \Set{v} $$
\paragraph{Edge Reversal}
Reverse edge from node $u$ to node $v$. Denoted as $\revedge{u, v}$. $$ \affnodes{\revedge{u, v}} = \Set{u, v} $$

\subsubsection{Speed concerns}
The most expensive operation in the local search is the computation of $\score{X_i}$. Hence, we want to minimize the number of such computations.

\subsection{Greedy Local Search}
\subsubsection{Algorithm description}
At each step of the algorithm, we choose the local operation with maximal score and apply it. The algorithm terminates where there are no operations that increase the score. That means that it has reached a local optimum (relative to the given score and local operations set).

\subsubsection{Speed optimization}
\paragraph{Finding maximum score} In order to find an operation with maximal score efficiently, we can use data structures such as a binary heap. That way, we can retrieve an operation with maximum score in $\bigO{1}$ time.

However, when we apply the operation, several problems arise. Firstly, some operations in the heap start violating acyclity constraints. This problem can be solved by checking for acyclity when the operation is retrieved from the heap. 

Secondly, the score of some operations changes. The number of such operations is $\bigO{K}$. Hence removing and re-inserting them to the heap would require $\bigO{K \log{N_{op}}}$ time. 

\begin{algorithm}[t]
	\caption{Greedy Local Search algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{GreedyLocalSearch}{$\graph{G}_0, \score{\cdot}, \ops{\cdot}$}
		\State $\graph{G} \gets \graph{G}_0$
		\While{$\exists o \in \ops{\graph{G}}: \score{o} > 0$}
		\State $\displaystyle o \gets \argmax_{o \in \ops{\graph{G}}} {\score{o}}$
		\State $\graph{G} \gets o(\graph{G})$
		\EndWhile
		\State \textbf{return} $\graph{G}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Graph scores and mutual information}
\paragraph{Mutual Information} Mutual information is defined as:
\begin{equation}
I(X; Y) = \int_{x \in X}{\int_{y \in Y}{p(x, y)\,\log \frac{p(x,y)}{p(x)\,p(y)}\,dx\,dy}}
\end{equation}
It can also be expressed as $KL$-divergence between distributions $p(x, y)$ and $p(x)p(y)$:
\begin{equation}
I(X; Y) = D_{KL}(p(x, y)\,||\,p(x)p(y))
\end{equation}
Note that $p(x, y) = p(x)p(y)$, and therefore $I(X; Y) = 0$, when $X$ is independent of $Y$. 

\paragraph{Application} Often computing graph scores for structure learning problem requires mutual information estimation. For example, BIC score. The reason behind this is that in discrete case, likelihood of a Bayesian network structure can be computed using mutual information:
\begin{equation}
\log p(\graph{G}|X) = N \sum_{i=1}^M{I(X_i; Pa(X_i))} - N \sum_{i=1}^M{H(X_i)}  
\end{equation}
where $N$ is the number of data points, $M$ is the number of variables.
\paragraph{Entropy} 
Mutual information, on the other hand, can be computed via entropy:
\begin{equation}
\label{eq:mi_via_entropy}
I(X; Y) = H(X) + H(Y) - H(X, Y)
\end{equation}
Where $H(X)$ is Shannon entropy:
\begin{equation}
H(X) = \int_{x \in X}{p(x)\,\log p(x)\,dx}
\end{equation}
We use Equation \ref{eq:mi_via_entropy} for computing mutual information because it allows us to cache $H(X)$ instead of $I(X; Y)$; since it can be used for computing several $I(X; Y)$, this scheme is more efficient.

\section{Fast score comparison}
\subsubsection{CPD posterior}
\paragraph{CPD estimation} Suppose we have a discrete random variable which takes on values $v_1, v_2, \ldots, v_m$ with probabilities $p_1, p_2, \ldots, p_m$ (these are the true probabilities). Now we have a dataset $\mathcal{D} = (x_1, x_2, \ldots, x_N)$, and we want to estimate the true probabilities $p_i$.

By the Bayes theorem,
\begin{equation}
p(p_1, \ldots, p_m | \mathcal{D}) = \frac{p(p_1, \ldots, p_m)}{p(\mathcal{D})} p(\mathcal{D} | p_1, \ldots, p_m) 
\end{equation}

$p(\mathcal{D})$ is a constant w.r.t. $p_1, \ldots, p_m$.
\begin{multline}
p(\mathcal{D}|p_1, \ldots, p_m) = \prod_{i=1}^{N}{p(x_i|p_1, \ldots, p_m)} = \prod_{i=1}^{N}{p_{x_i}} = \prod_{i=1}^{N}{p_1^{[x_i = 1]} p_2^{[x_i = 2]} \ldots p_m^{[x_i = m]}} = \\ p_1^{\sum_{i=1}^N{[x_i = 1]}} p_2^{\sum_{i=1}^N{[x_i = 2]}} \ldots p_m^{\sum_{i=1}^N{[x_i = m]}} = p_1^{n_1} p_2^{n_2} \ldots p_m^{n_m} = \prod_{i = 1}^m{p_i^{n_i}}
\end{multline}

We assume that our prior on the probabilities is a Dirichlet distribution, i.e. it has the form 
\begin{equation*}
p(p_1, \ldots, p_m) \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_m)
\end{equation*}

\begin{equation}
p(p_1, \ldots, p_m) = \frac{1}{B(\alpha_1, \alpha_2, \ldots, \alpha_m)} \prod_{i=1}^m{p_i^{\alpha_i - 1}}
\end{equation}

The uniform prior corresponds to $\alpha_i = 1$ assignment.

Combining these equations, we have that the posterior over $p_1, p_2, \ldots, p_m$ is also a Dirichlet distribution
\begin{multline}
p(p_1, p_2, \ldots, p_m|\mathcal{D}) = \frac{1}{Z_1}\,p(p_1, \ldots, p_m)\,p(\mathcal{D}|p_1, \ldots, p_m) = \\ \frac{1}{Z_2} \, \prod_{i=1}^m{p_i^{\alpha_i - 1}} \, \prod_{i=1}^m{p_i^{n_i}} = \frac{1}{Z_2}{ \prod_{i=1}^m{p_i^{n_i + \alpha_i - 1}}}
\end{multline}

\begin{equation*}
p(p_1, p_2, \ldots, p_m|\mathcal{D}) \sim \textrm{Dirichlet}(\alpha_1 + n_1, \alpha_2 + n_2, \ldots, \alpha_m + n_m)
\end{equation*}

\subsubsection{Dirichlet distribution}

\paragraph{Probability density function}
\begin{equation}
p(p_1, \ldots, p_m) = \frac{1}{B(\alpha_1, \alpha_2, \ldots, \alpha_m)} \prod_{i=1}^m{p_i^{\alpha_i - 1}}
\end{equation}

\paragraph{Support}
\begin{equation}
p_1 + p_2 + \ldots + p_m = 1
\end{equation}

\paragraph{Marginals}
\begin{equation*}
\alpha_0 = \sum_{i=1}^m{\alpha_i}
\end{equation*}

\begin{equation*}
p(p_i) \sim \textrm{Beta}(\alpha_i, \alpha_0 - \alpha_i)
\end{equation*}

\begin{equation}
p(p_i) = \frac{1}{B(\alpha_i, \alpha_0 - \alpha_i)} \, p_i^{\alpha_i} (1 - p_i)^{\alpha_0 - \alpha_i}
\end{equation}

\paragraph{Mean}
\begin{equation}
\expect{p_i} = \frac{\alpha_i}{\alpha_0} = \frac{\alpha_i}{\sum\limits_{j=1}^m{\alpha_j}}
\end{equation}

\paragraph{Variance}
\begin{equation}
\mathbb{D}{p_i} = \frac{\alpha_i(\alpha_0 - \alpha_i)}{\alpha_0^2(\alpha_0 + 1)}
\end{equation}

\paragraph{Covariance}
\begin{equation}
\textrm{cov}(p_i, p_j) = \frac{-\alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)}
\end{equation}

\subsubsection{Heavy math}

\newtheorem{theorem}{Theorem}
\begin{theorem}
If $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_n)$, $x = p_i$, $\alpha_x = \alpha_i$, $y = p_j$, $\alpha_y = \alpha_j$ and $k_x, k_y, m_x, m_y$ are arbitrary nonnegative constants, then 
\begin{multline}
\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \,  \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + k_y)}
\end{multline}

where $\alpha_z = \alpha_0 - \alpha_x - \alpha_y$
\begin{proof}
	Let $z$ be the combined probability of all values $v_k$ except $v_i$ and $v_j$. Then
	
	\begin{equation*}
	x, y, z \sim \textrm{Dirichlet}(\alpha_x, \alpha_y, \alpha_z)
	\end{equation*}
	
	\begin{equation}
	p(x, y, z) = \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} z^{\alpha_z - 1}
	\end{equation}
	
	
	$x + y + z = 1$, so $z$ is a deterministic function of $x$ and $y$: 
	
	\begin{equation}
	p(x, y) = \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1}
	\end{equation}
	
	\begin{multline*}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \int\limits_{0 < x + y < 1} {p(x, y) \, x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
	\\ \int\limits_{0 < x + y < 1} {\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_{0 < x + y < 1} { x^{\alpha_x + k_x - 1} y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log^{m_x} x \log^{m_y} y \, dx \, dy} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \int\limits_0^{1-x}{ y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log^{m_y} y \, dy}} = 
	\\  \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \int\limits_0^{1-x}{ \frac{\partial^{m_y}}{(\partial \alpha_y)^{m_y}} (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy}} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, dx \, \frac{\partial^{m_y}}{(\partial \alpha_y)^{m_y}} \int\limits_0^{1-x}{  (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy}} 
	\end{multline*}
	
	Now let's compute $I(x) = \int\limits_0^{1-x}{  (y^{\alpha_y + k_y - 1} (1 - x - y)^{\alpha_z - 1}) \, dy} $. Let $y = (1 - x)t$, then
	
	\begin{multline*}
	I(x) = \int\limits_0^1{((1 - x)t)^{\alpha_y + k_y - 1} (1 - x - (1 - x)t)^{\alpha_z - 1} \, d((1 - x)t)} = \\ \int\limits_0^1{(1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, t^{\alpha_y + k_y - 1} (1 - t)^{\alpha_z - 1} \, dt} = \\ (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \int\limits_0^1{t^{\alpha_y + k_y - 1} (1 - t)^{\alpha_z - 1} \, dt} = (1 - x)^{\alpha_y + \alpha_z + k_y - 1} B(\alpha_y + k_y, \alpha_z)
	\end{multline*}
	
	\begin{multline*}
	\mathbb{E}(x^{k_x} y^{k_y} \log^{m_x} x \log^{m_y} y) = \\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_y}{x} \, \frac{\partial^{m_y} (1 - x)^{\alpha_y + \alpha_z + k_y - 1} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y}} \, dx } = 
	\\ \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { x^{\alpha_x + k_x - 1} \log^{m_x}{x} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \int\limits_0^1 { \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} (x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)}) \, dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \, \int\limits_0^1 {x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, \log^i{(1 - x)} \, dx }} = 
	\\ \sum_{i=0}^{m_y}{ \frac{C^i_{m_y}}{B(\alpha_x, \alpha_y, \alpha_z)} \, \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x}}{(\partial \alpha_x)^{m_x}} \,
		\frac{\partial^{i}}{(\partial \alpha_y)^{i}} \, \int\limits_0^1 {x^{\alpha_x + k_x - 1} \, (1 - x)^{\alpha_y + \alpha_z + k_y - 1} \, dx }} = 
	\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \,  \sum_{i=0}^{m_y}{ C^i_{m_y} \frac{\partial^{m_y-i} B(\alpha_y + k_y, \alpha_z)}{(\partial \alpha_y)^{m_y-i}} \, \frac{\partial^{m_x + i}}{(\partial \alpha_x)^{m_x} (\partial \alpha_y)^{i}} \, B(\alpha_x + k_x, \alpha_y + \alpha_z + k_y)}
	\end{multline*}
\end{proof}
\end{theorem}

\begin{theorem}
If $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_n)$, $x = p_i$, $\alpha_x = \alpha_i$, $y = p_j$, $\alpha_y = \alpha_j$, then

\begin{equation}
\mathbb{E}\log(x + y) = -\sum_{n=1}^{\infty}{\frac{1}{n} \frac{B(\alpha_x + n, \alpha_y + \alpha_z)}{B(\alpha_x, \alpha_y + \alpha_z)}}
\end{equation}

\begin{proof}

\begin{multline}
\mathbb{E}\log(x + y) = \int\limits_{0 < x + y < 1}{\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(x + y) \, dx \, dy} = 
\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \int\limits_{0 < x + y < 1}{ x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, \log(1 - (1 - x - y)) \, dx \, dy} = 
\\ \frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \int\limits_{0 < x + y < 1}{ x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z - 1} \, (-\sum_{n=1}^{\infty}{\frac{(1 - x - y)^n}{n}}) \, dx \, dy} =
\\ -\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} \int\limits_{0 < x + y < 1}{ x^{\alpha_x - 1} y^{\alpha_y - 1} (1 - x - y)^{\alpha_z + n - 1} \, dx \, dy}}
\end{multline}

Note that the integral has the same form as in the proof of Theorem 1, with $k_x = m_y = m_x = k_y = 0$, so we have
	
\begin{equation*}
\mathbb{E}\log(x + y) = -\frac{1}{B(\alpha_x, \alpha_y, \alpha_z)} \, \sum_{n=1}^{\infty}{\frac{1}{n} B(\alpha_y, \alpha_z + n) B(\alpha_x, \alpha_y + \alpha_z + n)} 
\end{equation*}	

\end{proof}
\end{theorem}

\subsubsection{Entropy variance}
\paragraph{Entropy} Recall that the entropy has the form
\begin{equation*}
H(p_1, p_2, \ldots, p_m) = \sum_{i = 1}^m{p_i \log p_i}
\end{equation*}

We want to compute the variance of our estimate of the entropy $\mathbb{D}H(p_1, p_2, \ldots, p_m)$, where $p_1, p_2, \ldots, p_m \sim \textrm{Dirichlet}(\alpha_1 + n_1, \ldots, \alpha_m + n_m)$, as discussed above.

\begin{equation}
\mathbb{D}(\sum_{i = 1}^n{X_i}) = \sum_{i=1}^n{\mathbb{D}X_i} + 2 \sum_{i = 1}^n{\sum_{j = 1, \\ j < i}^n{{\textrm{cov}(X_i, X_j)}}}
\end{equation}

Applying this formula to our task, we have that
\begin{equation}
\mathbb{D}(\sum_{i = 1}^n{p_i \log{p_i}}) = \sum_{i=1}^n{\mathbb{D}(p_i \log{p_i})} + 2 \sum_{i = 1}^n{\sum_{j = 1, \\ j < i}^n{{\textrm{cov}(p_i \log{p_i}, p_j \log{p_j})}}}
\end{equation}

A well-known fact from probability theory states that
\begin{equation}
\mathbb{D}X = \mathbb{E}(X - \mathbb{E}X)^2 = \mathbb{E}(X^2 - 2X \mathbb{E}X - (\mathbb{E}X)^2) = \mathbb{E}(X^2) - (\mathbb{E}X)^2
\end{equation}

\begin{multline}
\textrm{cov}(X, Y) = \mathbb{E}(X - \mathbb{E}X)(Y - \mathbb{E}Y) = \\ \mathbb{E}(XY - X \mathbb{E}Y - Y \mathbb{E}X + \mathbb{E}X\mathbb{E}Y) = \mathbb{E}XY - \mathbb{E}X\mathbb{E}Y
\end{multline}

Applying,
\begin{equation}
\mathbb{D}(p_i \log{p_i}) = \mathbb{E}(p_i^2 \log^2{p_i}) - (\mathbb{E} \, p_i \log{p_i})^2 
\end{equation}
\begin{equation}
\textrm{cov}(p_i \log{p_i}, p_j \log{p_j}) = \mathbb{E}(p_i p_j \log{p_i} \log{p_j}) - (\mathbb{E} \, p_i \log{p_i})(\mathbb{E} \, p_j \log{p_j})
\end{equation}

Note that all values on the right side can be computed using Theorem 1. Hence, we have an analytical formula for $\mathbb{D}H(p_1, p_2, \ldots, p_m)$.

\subsubsection{Mutual information variance}
Mutual information can be expressed as  
\begin{equation}
MI(X, Y) = H(X) + H(Y) - H(X, Y)
\end{equation}
where $X$ and $Y$ are some sets of random variables. 

In local structure search, we need to estimate
\begin{equation}
MI(X, Pa(X)) = H(X) + H(Pa(X)) - H(X, Pa(X))
\end{equation}

In terms of $p_{11}, p_{12}, \ldots, p_{nm}$ -- probabilities of instantiations of $(X, Pa(X))$:
\begin{multline}
MI(p_{11}, p_{12}, \ldots, p_{nm}) = \sum_i{(\sum_j{p_{ij}}) \log(\sum_j{p_{ij}})} + \sum_j{(\sum_i{p_{ij}}) \log(\sum_i{p_{ij}})} - \\ \sum_{i,j}{p_{ij} \log p_{ij}}
\end{multline}

In order to estimate the variance of mutual information we have to compute the variance of every term of this sum (it can be done by applying Theorem 1 directly), and covariances of all pairs of terms.

Covariances between terms that lie in the same part of the expression can also be computed by applying Theorem 1, as it was done in the previous section.

Covariances of terms that lie in part 1 and part 3, or part 2 and part 3 can be computed using Theorem 2.

\section{Mutual information estimation as a statistical test}
\paragraph{TODO} G-test and p-value estimation via moments (as opposed to chi-squared approx.)

\section{Testing and benchmarking}
\subsection{Tiling Bayesian networks \cite{Tsamardinos2006}}

\begin{thebibliography}{9}
	
	\bibitem{KollerFriedman}
	Daphne Koller, Nir Friedman
	\emph{Probabilistic Graphical Models: Principles and Techniques},
	The MIT Press, Cambridge, Massachusetts,
	2009.
	
	\bibitem{Tsamardinos2006}
	Ioannis Tsamardinos, Alexander Statnikov, Laura E. Brown, Constantin F. Aliferis
	\emph{Generating realistic large bayesian networks by tiling},
	2006.
	
	
	
\end{thebibliography}

\end{document}

